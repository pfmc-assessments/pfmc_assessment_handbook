[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unofficial NWFSC and SWFSC Groundfish Stock Assessment Handbook",
    "section": "",
    "text": "Preface\nNote: this is a document for use by the NWFSC and SWFSC groundfish stock assessment groups. Nothing in this document is guaranteed to be accurate or up-to-date, to work for any particular species, to write your model for you, or to save you from getting grilled at a STAR panel.\n\nDear Reader,\nThis document is not intended to be followed blindly. Its purpose is to collect in an organized and accessible format some of the wisdom that has been accumulating in the heads of stock assessment authors (see list of past assessments). Where reasonable alternative approaches can be used, they should be listed here, with indications of the pros and cons of each approach. If you think something is missing, please add it–this is a work in progress. If you would prefer that assessment authors all make independent, arbitrary decisions, uninformed by what’s been done in the past, and unaware of the issues associated with different approaches, feel free to stop reading this document at any time. Otherwise, please don’t criticize these notes in public meetings.\n\n\nAlternative sources for best practices\nThe SSC Groundfish Sub-committee has produced documents “ACCEPTED PRACTICES GUIDELINES FOR GROUNDFISH STOCK ASSESSMENTS”. These documents, as well as the official groundfish stock assessment terms of reference, are both more up-to-date than this handbook and also carry more weight. However, they are also less comprehensive than this document. You should either follow their guidelines or be prepared to justify why you didn’t. The accepted practices documents and the terms of reference are available at available at https://www.pcouncil.org/stock-assessments-star-reports-stat-reports-rebuilding-analyses-terms-of-reference/groundfish-stock-assessment-documents/ (search for “accepted” or “terms”).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-data-sources.html",
    "href": "01-data-sources.html",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "",
    "text": "1.1 Table of contacts\nContact info for various agencies (states, PacFIN, etc.) is listed in this Google Doc created by Jason Cope in 2009 (updated May 2021)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "01-data-sources.html#requesting-data-from-state-partners",
    "href": "01-data-sources.html#requesting-data-from-state-partners",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.2 Requesting data from State partners",
    "text": "1.2 Requesting data from State partners\nStarting with the 2023 cycle, there is a more formal process for making data requests to states for stock assessments. Details about the data requesting process, and steps for assessment leads to take, can be found in the document “Process for requesting data from states for 2023 cycle.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "01-data-sources.html#regulations-and-catch-limits",
    "href": "01-data-sources.html#regulations-and-catch-limits",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.3 Regulations and catch limits",
    "text": "1.3 Regulations and catch limits\nJim Hastie has lived through many changes in regulations for west coast groundfish. Don Pearson has recently (Fall of 2014) created this online regulations database.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "01-data-sources.html#catch-data-lengths-ages-etc.",
    "href": "01-data-sources.html#catch-data-lengths-ages-etc.",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.4 Catch data, lengths, ages, etc.",
    "text": "1.4 Catch data, lengths, ages, etc.\n\n1.4.1 West Coast Groundfish Observer Program (WCGOP)\nSee section below on “Notes and best practices for observer data and discards” ADD LINK\n\n\n1.4.2 At-Sea Hake Observer Program (ASHOP)\nVanessa Tuttle (Vanessa.Tuttle@noaa.gov) is the contact person for this. Unlike the shoreside subset of the hake fishery which has dockside sampling and bycatch reported in PacFIN, the observed bycatch in the at-sea sectors (motherships and catcher-processors) is reported to the NORPAC database housed at AFSC. A non-definitive list of “common” and “uncommon” species seen in the hake fishery (as of 2008, but Vanessa says it is still current as of 2022) is in Appendix A of this document.\n\n\n1.4.3 PacFIN biological data (BDS)\nTo get these data, create a “Data request” issue on the pacfintools github repo. To confirm sample sizes for BDS, go to this webpage which should give a recent tally of samples by state agency.\nGeneralized code to process the data are currently in {pacfintools}.\nYou may see some discrepancies in CA data from 1985-1989 when compared to older extractions. Some samples were removed for some species. It is not clear why these samples were removed.\n\n\n1.4.4 PacFIN landings\nTo get these data, create a “Data request” issue on the pacfintools github repo.\nNote that the PacFIN catch can be broken down by INPFC or month only for the trawl component, not the non-trawl gears. Also, beware double counting based on multiple levels of aggregation.\nPSMFC areas do not contain all of the catch, thus it is best to use INPFC areas to aggregate catch.\nOnce you have catch values for all years, contact the state representative(s) for confirmation that the values are correct.\n\n\n1.4.5 RecFIN\nAll states have in the past claimed that all rec. data is available on RecFIN. Users should check with state representatives for updated information on the recommended source for recreational data. One can pull historical catches here. Select the “Catch / Sample Data Reports” image which takes you to the reports dashboard. Alternatively, ODBC connections can be used to pull catch estimates directly from the Comprehensive Fish Ticket table. Some data sources (e.g. MRFSS, WA historic catch estimates) are not currently available via the public login, so you need to login with an account that has permission to view the tables. Additionally, once you have catch values for all years, contact the state representative for confirmation that the values are correct.\nWashington provides historical recreational catch in numbers. (Modern catches are available in both numbers and weight.) This leads to two decisions for putting catches into the model: keep catch in numbers, which stock synthesis converts to weights internally; or convert catch to weight using an external estimate. Both options have been done historically. The former causes challenges for projections, but if the former is chosen, it is useful to compare the model’s calculated average weight of the catch to the value used to convert numbers to weight to see if there is large disagreement. For a partial history about this issue, and approaches for some past assessments see issue #52 from the canary rockfish github page. Note that Washington historical catches are publicly available on RecFIN, but should be filtered to areas 1-4 (coastal marine catch areas). As of the 2025 yellowtail assessment, publicly available RecFIN catch data excluded all Puget Sound catch, whereas catch from the “Sekiu and Pillar Point” area near the entrance to Puget Sound should be included in assessments in federal waters. In order to include these catches, you will need to use the confidential catch data table (CTE501), rather than the public version (CTE001), which aggregates data to a higher spatial resolution and excludes Puget Sound.\nTo obtain length composition data,\n\nGo to RecFIN\nSelect the “Catch / Sample Data Reports” image which takes you to the reports dashboard. There, select the “SD001 Biological Detail Report” option.\nThere is an automatic filtering applied, so to adjust select the ‘filter’ icon in the upper right (the upside down Erlenmeyer flask-like icon), and then download your data in either csv or excel format. Lengths come in imputed and measured, with T being total length and F being fork length. For questions on definitions of fields, the metadata is included as a selectable report in the reports dashboard.\n\nE.J. Dick (SWFSC) and Jason Edwards (PSMFC) are developing tables with catch-weighted length compositions. Tables have been prepared for WA and OR, and CA is in prep. Contact E.J. (edward.dick@noaa.gov) if you have questions or would like to use the estimates.\n\n\n1.4.6 Research catch\nThis is not automatically included in any of the other data sources. Gretchen Hanshew (Gretchen.Hanshew@noaa.gov) has been the source in the past. Talk to John Wallace and Ian Taylor about complexities regarding PacFIN records of landings from research catch.\n\n\n1.4.7 Foreign landings\nForeign commercial landings in U.S. waters that occurred prior to closure of the EEZ are not included in any state historical reconstruction, but should be included in catch time series. Estimates for a number of Sebastes and Sebastolobus species are available in Rogers 2003.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "01-data-sources.html#indices-of-abundance",
    "href": "01-data-sources.html#indices-of-abundance",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.5 Indices of abundance",
    "text": "1.5 Indices of abundance\nAt a team meeting on 13 March 2019, the team agreed on common terminology for the surveys, where best practice would be to introduce the survey initially using the full name, with the short name in parentheses, and then use the short name after that. These names have been slightly revised to better match the names used in the Miller et al. background doc “Overview of West Coast Groundfish Fishery-Independent Surveys” (available to NOAA folks on Google Drive at here)\n\nNWFSC West Coast Groundfish Bottom Trawl Survey (WCGBT Survey or WCGBTS)\nNMFS West Coast Triennial Shelf Survey (Triennial Survey)\nAFSC West Coast Slope Survey (AFSC Slope Survey)\nNWFSC West Coast Slope Survey (NWFSC Slope Survey)\nNWFSC Southern California Shelf Rockfish Hook and Line Survey (NWFSC HKL Survey)\nNWFSC Integrated Acoustic and Trawl Survey of Pacific Hake (Acoustic Survey)\n\nNote: the names for the first 4 of these surveys in the {nwfscSurvey} package are “NWFSC.Combo”, “Triennial”, “AFSC.Slope”, “NWFSC.Slope”. The HKL and acoustic surveys are not currently available through that package.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "01-data-sources.html#summary-of-noaa-fishery-independent-trawl-surveys-used-for-west-coast-assessments",
    "href": "01-data-sources.html#summary-of-noaa-fishery-independent-trawl-surveys-used-for-west-coast-assessments",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.6 Summary of NOAA fishery independent trawl surveys used for west coast assessments",
    "text": "1.6 Summary of NOAA fishery independent trawl surveys used for west coast assessments\n\nTriennial Survey (1980–1992 & 1995–2004)\n\nearly triennial (1980–1992, 55-366m, north of 36.8 (however the switch to north of 34.5 actually occurred in 1989))\nlate triennial (1995–2004, 55-500m, north of 34.5)\nThe 2007 Canary assessment (Stewart, 2007) seems to have been the first one to split the survey and includes info on justification (but there’s an ongoing debate about the need to split).\nMust filter out water hauls and tows occurring outside the US EEZ (foreign tows)\nNote that 1977 is always tossed out.\nThe 2005 and 2013 Shortspine Thornyhead assessments split the triennial into shallow vs. deep to create a single index from 1980-2004 for 55-366 m and a separate 366-500 m index for 1995-2004.\nStrata information from the 2001 AFSC Triennial Survey Plan includes the following information on the depth strata “Sampling density will be similar throughout the three depth strata: 30-100, 101-200, and 201-275 fm (55-183, 184-366, and 367-500 m) of the survey area. Tracklines are spaced at 10 nmi intervals and stations are located randomly along those tracklines at the rate of one station per four nmi of linear distance in the shallow stratum and one station every five nmi of linear distance in the two deeper strata.”\n\nAKFSC Slope Survey (1997-2001, 183–1280m, north of 34.5)\n\nYears before 1997 surveyed small areas of the coast\nMust filter out tows occurring outside the US EEZ\n\nNWFSC Slope Survey (1998-2002, 183-1280m, north of 34.5)\nWCGBT Survey (2003-present, 55-1280m, entire US coast)\n\nStarting in 2004, there’s a change in sampling intensity north and south of 34.5, so this strata boundary should be included (unless there’s some specific reason not to)\nNote that there were changes in sampling intensity at 183 and 549 meters\nThis survey should be referenced as the “WCGBT Survey”\n\n\n\n\n\nSummary of the day of the year, latitude and depth covered by each of the primary trawl surveys on the U.S. West Coast. Values that are typically excluded (Triennial in 1977 and AFSC Slope survey prior to 1997) are shaded gray. Note that the depth figure uses a log2-scale to better reflect the relative area associated with different depth ranges (shelf is generally wider than slope).\n\n\n\n1.6.1 AFSC surveys\nData are now available through the NWFSC data warehouse which can be accessed by the functions in the {nwfscSurvey} package. Appendix B identifies which year the listed vessels participated in the Triennial and Slope Surveys. Each cruise is assigned a unique number which is contained within the ‘CRUISEJOINS’ column in the database. Checking that your data has all the cruises you expect for a given survey would be good practice. Although gear and personnel change over time, if a study looking at differences by vessel was undertaken, Appendix B shows which vessels participated in what year and survey.\n\n1.6.1.1 More detail on these AFSC surveys\nThe the most recent RACE division species and data codes manuals are here.\nThe ADP Code Book has, for example, sex and performance code information. For convenience, here is the legacy coding for sex:\n\nSex\n1 Male\n2 Female\n3 Undetermined\n\nNote on design of the 2004 Triennial Survey\n\nIt is my (John Wallace) understanding that in the later years of Triennial survey (pre-2004), the survey became more of a fixed survey design as the skippers went back to the same locations as recorded on their vessel’s instrumentation.\nFor the 2004 survey, I followed the design as put forth in:\n2001 AFSC Triennial Survey Plan (converted from WordPerfect via Word)\nwithout regard to any previously recorded tow locations.\nFor CRUISEJOIN info, see Appendix B.\n\n\n\n\n1.6.2 NWFSC Survey Indices\nTech memo on “history, design, and description” of the survey is now available:\n\nhttps://www.nwfsc.noaa.gov/assets/25/8655_02272017_093722_TechMemo136.pdf\nKeller, A. A., J. R. Wallace, and R. D. Methot. 2017. The Northwest Fisheries Science Center’s West Coast Groundfish Bottom Trawl Survey: History, Design, and Description. U.S. Department of Commerce, NOAA Technical Memorandum NMFS-NWFSC-136. DOI: 10.7289/V5/TM-NWFSC-136.\n\nAdditional information on the survey can be found in these documents from John Wallace:\n\nStrata Tow Percentages for NWFSC Bottom Trawl Survey for 2004-Current\nCalcs for Strata Tow Percentages and Station Selection for the NWFSC Bottom Trawl Surveys for 2003 and Beyond (PDF file on Google Drive)\n\nAs of January 2025, index standardization for the trawl surveys is conducted by a subset of NWFSC staff used the {indexwc} package and shared with the assessment authors. The documentation of this package will be improved in the future to make it easier for folks to explore alternative indices beyond what is initially provided.\n\n\n1.6.3 NWFSC Survey Length and Age Compositions\n{nwfscSurvey} package is used for comp data and other data explorations.\nScaling from Tow to Stratum Level: Weight normalized length or age comps for each tow by the numerical CPUE. This is done in the standard data package we get from Beth.\nScaling from Stratum to Coastwide (or Assessment Area) level: Weight strata length or age comps by numerical index for each strata (from GLMM). This may mean dividing the biomass index for each stratum by the average weight in that stratum (likely estimated from the length comp in that stratum).\n\n1.6.3.1 Filtering recorded catches that are fish stuck in net from previous tow\nIn some cases, fish caught in one tow remain in the net until the next tow and are recorded as caught in that second tow (despite the attempts of the people on the survey to identify and exclude such fish from the data). While we plan to come up with a consistent way to deal with this, it has not been dealt with yet.\nOne problem arising from this is catch data outside of the depth range of the species. To identify the depth range, one can start with the deepest/shallowest tow with a positive record for that species and then look to see if the previous tow conducted by that vessel (likely on the same date) caught that species. If so, and if a small number or a small relative number of that fish species were recorded in the second tow, one can assume that those were from the previous tow. By moving sequentially until one reaches a depth range where a few clearly legitimate tows occurred, one can define a reasonable depth range.\nA second phase would be to define a way to filter all the tows to remove likely candidates. A simple way to do this is to look at the range of catch levels that were determined to be not legitimate, and filter the whole data set for that species by removing all tows with catches below some quantile of those catch levels (say 80 or 90%). This will eliminate having a large number of small catches in the database used for the GLMM, which would skew the modeled distribution.\n\n\n\n1.6.4 IPHC survey\nThis survey has been used for Yelloweye Rockfish and Spiny Dogfish. Claude Dykstra (claude@iphc.int) has provided data in the past. The index was calculated using a binomial GLM developed by John Wallace. WDFW staff have also explored standardizing this index (work for Yelloweye in 2025 still in progress).\n\n\n1.6.5 Oregon recreational observer program data\nTroy Buell (troy.v.buell@state.or.us) is still involved with this data source even though he has changed positions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards",
    "href": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.7 Notes and best practices for observer data and discards",
    "text": "1.7 Notes and best practices for observer data and discards\nDiscard mortality by fleet should be accounted for within the assessment. To obtain estimates of total discards based on the fleet structure in the model (gear and/or area) see the Groundfish Expanded Multi-year Mortality (GEMM) report which is a component of the annual Groundfish Mortality Report. The GEMM provides annual estimates of landed and discarded fish by sector off the West Coast starting in 2002 - present. However, the GEMM will not include the most full recent year estimates (example - the GEMM available in 2019 has data through 2017) which will require an assumption regarding the most recent year’s discards. Additionally, assumptions will need to be made regarding the total amount of discard prior to the start of the WCGOP data (pre-2002).\nThe GEMM report is produced by the WGCOP team with Dr. Kayleigh Somers (kayleigh.somers@noaa.gov) as the lead author and is published in the September Council meeting Briefing Book titled the “Groundfish Mortality Report”. The GEMM can be found within the associated Excel spreadsheet on the Table 3 tab. Additionally, the GEMM can be accessed by using the nwfscSurvey using the pull_gemm() function.\nThere are typically three common methods used within West Coast groundfish stock assessments to incorporate discard mortality.\nThe first approach is to include discard mortality into the catch data rather than estimating discards within the model. Discard mortality can be included by either adding this total to the catch by fleet or by adding a discard fleet. If using the discard fleet the selectivity of this fleet will need to be mirrored (see the 2017 California Blue/Deacon assessment for an example of this approach).\nThe second and third alternative approaches to including discard data is to model the process of discarding within SS3. Discards can be modeled either as total discards in mt or through the rate of discarding. Both of these approaches will require discard length data to be included in the model, or if not available, a specific assumption regarding the retention curve relative to the estimated selectivity curve.\nEstimates of total discards are available in the annual groundfish mortality report (and the GEMM). If a specific breakdown of discard by fleet is required (gear and/or area) speak with Dr. Chantel Wetzel. The data may not be able to be summarized to all fleet structures, so it is best to discuss the options with Chantel early in the assessment process. To get processed data, create a “Data request” issue on the nwfscDiscard github repo.\nAssumptions regarding the discard total in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of time blocks). If total discards are modeled, one will need to enter an annual CV into SS3 which is currently not available with the GEMM. The current approach for obtaining discard totals or discard rates with CV is via bootstrapping the available WCGOP observer data. The bootstrap CV will be based on the observer data and may not be entirely consistent with the data available in the groundfish mortality report (and the GEMM), particularly for sectors with limited observer coverage. Please see below for additional information regarding bootstrapping.\nModeling discard rates can be an easier approach since it does not require knowledge about the total discard amounts. However, this approach may not be feasible for stocks with limited observations in the WCGOP data due to high variability in the observed discarded and retained fish. The IFQ trawl fleet currently has near 100% observer coverage starting in 2011, while the non-catch share sectors have a much lower observed percentage (~20% as of last inquiry, but this may change in the future). Historically, it was thought that discard rates could not be calculated for complex managed species. However, this is not the case based on the current data. Discard rates are calculated based on the observed discarded weight relative to the total observed weight of discarded and retained fish. Bootstrapping is done using the data in order to provide a CV to the discard rate. Similar to two previous approaches, assumptions regarding the discard rate in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of time blocks).\nNote, if discard mortality is estimated within the model, it is important to compare the catches (landings + discards) estimated within SS3 to the total mortality estimates from the GEMM. The GEMM is the official mortality report for West Coast groundfish stocks as is used to determine the ACL and OFL attainment and if overfishing is occurring. The total mortality estimates between SS3 and the GEMM would not be expected to match exactly; however, the should have a reasonable level of similarity. If there are large differences between the mortality estimates across multiple years one may need to reconsider if the model has enough data to appropriately estimate retention reliably for each fleet. Discarding practices can easily be heterogenous within a fleet that appears homogeneous based on landings. For example, the Makah tribe has required that all overfished and commercially saleable rockfish be landed since the late 1990s, leading to essentially zero tribal discards. The discard rate calculated from WCGOP data does not apply to tribal vessels (which WCGOP does not observe), so if tribal landings are substantial, the use of a discard rate could overestimate total mortality.\nThe SSC has specified that the GEMM is the official data source to update recent removals when conducting a catch-only update model. The GMT is currently tasked with allocating removals by modeled fleet using the GEMM. The GMT will provide the removal estimates for year with GEMM data, provide expected removals for the final model year, and removal assumptions to use during the projection period. These data will typically be available between March and April of the assessment year.\nFinally, the GEMM includes recreational mortality by state. These values are not estimated by WCGOP but rather are pulled directly from RecFIN since WCGOP does not observe recreational fisheries.\n\n1.7.1 What are the WCGOP data?\nThe WCGOP database includes only data collected by observers on West Coast vessels. The data available in this database varies fairly dramatically based on the sector. The IFQ vessels essentially have 100% observer coverage, so the database includes a comprehensive view of discards and retained catch from observers for that sector. However, other sectors have a much lower percentage of observer coverage (see the observer coverage). As an example, the nearshore fishery generally has about 20% observer coverage. Only the observations made on that percentage of vessels are going to be available in the WCGOP database. Hence, the observed retained and discard amounts are going to be a significant underestimate of total mortality in that sector. The discard ratio will also be more variable, but could generally be representative of the discarding behavior.\n\n\n1.7.2 Available gear types\nWCGOP observers a wide-range of fishing sectors for vessels participating in the Individual Fishing Quota (IFQ, also referred to as catch share) fishery and non-catch share fisheries. Groundfish stock assessment most often define fleet structures within assessment models based on gear types. WCGOP data contains data for the following gear groups: bottom trawl, fixed gears, hook & line, midwater trawl, pot, and shrimp trawl. Grouping data based upon gear types will include data from multiple sectors which may or may not catch particular species. Each gear type includes the data from the following sectors:\n\nbottom trawl: catch shares, limited entry Pacific halibut, limited entry trawl, open access California halibut, and sea cucumber\nfixed gear: nearshore\nhook & line: catch shares, directed Pacific halibut, limited entry fixed gear DTL, limited entry sablefish, and open access fixed gear\nmidwater trawl: catch shares, limited entry trawl, midwater hake, and midwater rockfish\npot: catch shares, limited entry sablefish, and open access fixed gear\nshrimp trawl: pink shrimp and ridgeback prawn.\n\nThe groundfish expanded multi-year mortality (GEMM) report can be used to understand how much mortality (landed and discarded) is coming from each sector for any particular species. Examining these data can help analysts understand how best to use these data within their assessment. The GEMM data can be accessed using the pull_gemm() in the nwfscSurvey package.\nAs noted above, the observer coverage varies by sector with the catch share fishery have 100% observer coverage starting in 2011. Given that, these data are treated as a census with estimates of discarding rates provided separately between catch share and non-catch share data. Additionally, electronic monitoring became available for catch share vessels starting in 2015. Vessels using electronic monitoring have video coverage of on the water discarding rather than always having an human onboard observer. These data are also considered to be a full representation of discarding.\nThe catch share fishery includes the following sectors: catch share, catch share electronic monitoring, limited entry California halibut, midwater hake, midwater hake electronic monitoring, midwater rockfish, and midwater rockfish electronic monitoring.\n\n\n1.7.3 Bootstrapping\nThe current approach to obtain uncertainty around the data in the WCGOP database, either the total discard or discard rates, is to bootstrap the data. These data are summarized and the observations bootstrapped to obtain uncertainty estimated based on gear and area stratification requested by the assessor. Chantel Wetzel (chantel.wetzel@noaa.gov) currently conducts the bootstrap analysis.\n\n\n1.7.4 Biological data\nWGCOP contains data on the lengths of fish discarded by gear type and sector as well as the average body weight of discarded fish that can be used within an assessment to estimate discarding within the model.\nGenerally, retained length or margninal age composition data within assessments from the commercial fishery (PacFIN bds) or NWFSC surveys are expanded up to the tow/trip and up to the total catch weight by area. In contrast, WCGOP length or age data are only expanded up to the tow level. This is because the total discard or landed weight by area and gear is not readily available in the WCGOP data. Hence, analysts should be mindful about the observations by gear and sector to ensure that the overall compositions are representative of landed catch in the model by fleet. For example, if 50% of the discard lengths are arising from a sector with 5% of the overall landings in the model fleet, this could lead to a situation where the estimated retention curve that is not representative of the sector with the 95% of the landings in the model. Another example is the shrimp trawl fleet that generally has low overall bycatch of rockfish, but can have large bycatch numbers of small juvenile rockfish. Grouping these data together with other trawl gears will influence both the retention curve but potentially also the estimates of annual recruitment deviations. To deal with this one could choose to not group shrimp trawl length data within the trawl gear data or the analyst could request the length data separated by gear type and then weight the data by total discards available in the GEMM (e.g., one could also talk to Kayleigh Sommers if the correct stratification is not available in the GEMM).\n\n\n1.7.5 Requesting discard data\nThere are two types of discard data available using WCGOP data. The first is a summary of the observed discards, discard rates, and bootstrapped uncertainty across years (2002 - present). The second type of data are lengths of discarded fish observed by WCGOP. To request these data please create an issue on the nwfscDiscard github repo.\n\n\n1.7.6 Discard rates, and length comps, from the Pikitch et al. Discard Study (1985-87) and Mesh Study (1988-90) databases\nIf enough data exists, discard rates and length comps from the Pikitch et al. Discard Study database (or Mesh Study if no data exists in the Discard Study) may be obtained for a species to be assessed from John Wallace. Optionally, these rates can be expanded out to the surrounding years based on applying the study rates to PacFIN catch from the expanded years. This, of course, is only reasonable when the assumption of no significant changes to the fishery is a good one. For those in NOAA a draft of the methods paper can be downloaded here (for non-NOAA folks that are interested, contact the author at john.wallace@noaa.gov).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data sources: notes, contact information, and links</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html",
    "href": "02-model-choices.html",
    "title": "2  Modeling choices, notes and best practices",
    "section": "",
    "text": "2.1 Selecting maximum age (population and data)\nThe comments below are based on a team discussion on how to select a population and data maximum age for modeling:\nChoice 1, Population length bins: should extend beyond something like the 97.5% quantile of the length at age distribution for the largest age (easy to add more bins to see if it makes any difference).\nChoice 2, Max age: should be large enough for growth to be asymptotic, and at least as big as the largest data age bins. It’s easy to test the impact of changing to different value, just requires updating ageing error matrices. Look at the stable age-distribution plot and examine the number of fish in the plus group. Additionally, the period of time when data are available (after extensive exploitation), the data plus group may be lower.\nChoice 3, Data length bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. We should check to make sure that the selection of length and age bins are consistent with each other. Typically, we often have max length bins where there are only a few fish, but a larger proportion of data in the data age plus group.\nChoice 4, Data age bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. In regards to population maximum age, there is no negative repercussions within SS3 for having a larger value, beyond a slower run time. Revising the age bins based on the data, rather than a priori rules about how to set this, may be considered “data mining”. Could create a check within the survey and PacFIN comp codes that creates a flag when the data plus group has more than a certain percentage (i.e., 5%). Also, add a warning to {r4ss} about the percentage in the population and data plus groups.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#determining-prior-for-natural-mortality",
    "href": "02-model-choices.html#determining-prior-for-natural-mortality",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.2 Determining prior for natural mortality",
    "text": "2.2 Determining prior for natural mortality\nOwen’s Advice on \\(M\\); July 6, 2022\n\nI prefer using age data alone to estimate the natural mortality rate (see accompanying document: M2017_for_Methods_Review_Hamel; Hamel and Cope (in review)), except in cases where getting a reasonable estimate of the maximum age is problematic.\nThe age based prior is simply: \\[\\text{lognormal}(\\ln(5.4/\\text{max age}), 0.31).\\]\nThe fixed value for \\(M\\) if not estimating is even more simply the median of that prior: \\[M = 5.4/\\text{max age}\\]\nCan explore a range approaches for M estimates and priors here; reference is Cope and Hamel, in press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#length-weight-relationships",
    "href": "02-model-choices.html#length-weight-relationships",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.3 Length-weight relationships",
    "text": "2.3 Length-weight relationships\nUse the nwfscSurvey::estimate_weight_length(). It is common to rely only on data from the WCGBT Survey to get the parameters used in the model under the assumption that this survey is most representative of the population as a whole. These parameters should always be fixed in the model as no data type is available in SS3 to accurately estimate them internally. Other data sources (e.g., PacFIN if weights are available, etc.) can be passed to this function to estimate weight-length relationships. The col_length and col_weight arguments should be used to specify the specific string used to identify these quantities in your data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#estimating-ageing-error",
    "href": "02-model-choices.html#estimating-ageing-error",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.4 Estimating ageing error",
    "text": "2.4 Estimating ageing error\nSee the {AgeingError} package.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#maturity",
    "href": "02-model-choices.html#maturity",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.5 Maturity",
    "text": "2.5 Maturity\nTalk to Melissa Head about maturity. In 2017, she provided estimated maturity curves based on samples from the NWFSC survey for many species.\nData from the NWFSC survey on maturity includes a column indicating mature or immature and another indicating spawning and not spawning. The latter considers all “mature” fish with over 25% atresia as not spawning (along with all immature fish). The spawning/not spawning column is the one we commonly use to estimate the maturity curve since that is really what we care about. In some cases a simple logistic will fit, but if there is much skip spawning/atresia for older/larger females, a logistic type curve which asymptotes to a lower value or a non-parametric fit is more appropriate. A column with percent atresia is also provided if you with to use a percentage other than 25% for the cutoff. Finally, the mature/immature column can be used instead if the atresia/skip spawning is taken into account in specifying the fecundity relationship.\nAn additional column has been added to the NWFSC survey table indicating uncertainty in the designation. This can be used to weight or exclude data.\nNote: John Field has expressed concern that we are too focused on recent samples from the NWFSC survey, so if you aren’t going to include samples from past collections, think about a justification for that choice.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#fecundity",
    "href": "02-model-choices.html#fecundity",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.6 Fecundity",
    "text": "2.6 Fecundity\nDick et al. (2017) has estimates for the \\(a\\) and \\(b\\) parameters in the functional form \\(F = aL^b\\) for many rockfish. The estimates are based on length in mm and predicted number of eggs, fitted in log-space. If you use length in cm (like in SS3), and don’t want huge spawning biomass values (eggs), you can convert the values in the paper to units of cm and millions, billions, or trillions of eggs. First, find the values “\\(\\exp(a)\\)” (referred to as \\(a'\\) below) and “\\(b\\)” from Table 6 in Dick et al. for your species (or subgenus, if no estimate for your species is reported). If your subgenus is not reported, you can use the “Sebastes” values.\nThe correct value of the “\\(a\\)” parameter in Stock Synthesis (using fecundity at length option 2: eggs=a*L^b) with length in cm and spawning output in millions of eggs is:\n\\[a = \\frac{a'\\cdot10^b}{1000}\\]\nThe division is by 1 thousand instead of 1 million because recruitment in SS3 is in thousands. The value of \\(b\\) is unchanged, and can be used directly in the assessment.\nOther options include spawning output in billions of eggs via:\n\\[a = \\frac{a'\\cdot10^b}{10^6}\\]\nor spawning output in trillions of eggs (e.g. 2017 Yellowtail Rockfish):\n\\[a = \\frac{a'\\cdot10^b}{10^9}.\\]\nThe {r4ss} figures in the “Bio” tab show the resulting fecundity relationship as a function of length, weight, and age so you can use that to determine whether your parameter values produce the correct final relationship by comparing to original studies on the relationship between fecundity and length.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#modeling-selectivity",
    "href": "02-model-choices.html#modeling-selectivity",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.7 Modeling selectivity",
    "text": "2.7 Modeling selectivity\nThere was a CAPAM workshop on selectivity in 2013. Report, presentations, and recordings are available at here and the associated Fisheries Research special issue is here.\n\n2.7.1 Conceptual reasoning for using different approaches to selectivity\nThe following set of circumstances might cause selectivity to be dome-shaped:\n\nContact selectivity causing older fish to outswim the trawl, or escape the gillnet/hooks\nIncomplete spatial coverage in terms of depth or untrawlable habitat\nSpatial heterogeneity in fishing intensity (see Sampson paper). This probably applies more to fishery selectivity than surveys.\n\nReasons for justifying asymptotic selectivity\n\nIt can help estimate \\(L_\\infty\\) and variability in growth because the mode of the length comps is often representative of where the oldest fish are piling up.\nIt prevents the estimation of a large amounts of cryptic biomass\n\n\n\n2.7.2 General advice on selectivity in SS3\n\nStart with a functional form that is commonly used (double normal)\nFind some initial values using either the Excel spreadsheets or the {r4ss} widgets\nPut the initial values in the model and run without estimating anything until you get a model that runs. This can be done by setting the maximum phase in the starter file to 0 or (better), by using the command line inputs: -maxfn 0 -nohess.\nRead the output into R using SS_output and plot the selectivity in {r4ss} using either SS_plots(model, plot=2) or SSplotSelex(model, subplot=1), where model is the object created by SS_output().\nSet the PHASE for as many parameters to negative values as possible so that you start with estimating relatively few parameters (such as parameters 1 and 3 of the double normal, which control the peak and ascending slope).\n\n\n\n2.7.3 Guidelines for SS3 double normal initial setup\n\nFix parameters 5 and 6 (negative phase).\n\nIf selectivity is thought to be zero at the youngest/smallest or the oldest/biggest fish set the value to zero (e.g -15)\nIf selectivity is thought to be larger than zero at the youngest/smallest or the oldest/biggest fish set the value to -999 (does not scale the selectivity for the youngest or oldest age, independently from the normal curve).\n\nFix the plateau (parameter 2) to be small values (e.g. -15).\nSet the initial value for the peak (parameter 1) at the age/length equal to the mode of the composition data\nSet the ascending (parameter 3) and descending (parameter 4) slopes at \\(log(8 \\cdot (a_{peak}-a_{min}))\\) and \\(log(8 \\cdot (a_{max}-a_{peak}))\\) (substitute min and max lengths and length at peak when modeling length-base selectivity).\nDon’t estimate selectivity at youngest age/size (parameter 5) unless there are observations of fish in the smallest age- or length-bins, either fix at -5 or -999\nUse the double normal instead of the logistic for asymptotic selectivity to have flexibility of dome shape without major changes to control file. This also provides control over selectivity at the youngest age. To force a logistic shape, you can do one of the following (where parameters 2, 4, and 6 should not be estimated under any of the options):\n\nFix descending slope (parameter 4) at a large number (e.g. 15)\nAlternative 1: Fix plateau (parameter 2) to a large number (e.g. 15)\nAlternative 2: Fix selectivity of the oldest age (parameter 6) at a large number (e.g. 15).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#modeling-recruitment-deviations",
    "href": "02-model-choices.html#modeling-recruitment-deviations",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.8 Modeling recruitment deviations",
    "text": "2.8 Modeling recruitment deviations\n\nChoices to be made:\n\nallow recruitment deviations or not?\nrange of years?\nbreaking into “early”, “main”, “late” vectors?\n\nearly and late vectors are intended to add uncertainty to the model for years with little or no data with information about recruitment\n\n\nWhat was done in the 2011 assessments? Graphical description is here.\nGuidance on bias adjustment settings\n\nMultiple simulation analyses have shown that applying the bias adjustment settings given by the r4ss::SS_fitbiasramp() perform well on average. However, there’s no guarantee that this will work well in any given circumstance. User discretion is advised.\n\nWhat to do about \\(\\sigma_R\\).\n\nSimulation in Methot and Taylor (2011) looked at a few options.\n\nEstimating \\(\\sigma_R\\) . Performed well under ideal circumstances.\nTune \\(\\sigma_R\\) to match the observed variability in recruitment deviations. \\[ {\\sigma_R}^2 = \\text{sd}(r')^2 \\] Performed less well.\nTune \\(\\sigma_R\\) so that \\[{\\sigma_R}^2 = \\text{sd}(r')^2 + \\text{mean}(\\text{SE}(r'))^2\\] where \\(\\text{sd}(r')\\) is the standard deviation of the vector of estimated recruitments over a range of years that seem reasonably well informed by the data, and \\(\\text{mean}(\\text{SE}(r'))\\) is the mean of the estimated variability of those values. Performed best.\n\n\nMCMC\n\nIf you can get MCMC to work for your model, all the worry about bias adjustment goes away. You would need to either estimate \\(\\sigma_R\\) in the MCMC and hope it converges, or fix it at a value that has been tuned to the MLE.\n\nAutocorrelated recruitment deviations\n\nEstimate # SR_autocorr (Stock-Recruit parameter in control file)\nHere are results from hake in 2019 from the ss_new file: -1 1 -0.155106 0 99 0 6 0 0 0 0 0 0 0 # SR_autocorr\nJohnson et al. (2016) estimated autocorrelation in a simulation context",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#rockfish-steepness-profile",
    "href": "02-model-choices.html#rockfish-steepness-profile",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.9 Rockfish steepness profile",
    "text": "2.9 Rockfish steepness profile\nUp until the 2019 assessment cycle, a meta-analysis approach was used to develop a prior for steepness for West Coast rockfish species. This method was originally developed to develop this prior by Martin Dorn in 2007, and updated by Dorn in 2009/2011. It was then revised and recoded by James Thorson, who updated it in 2013/2015/2017, with Chantel Wetzel conducting the update in 2019. When the meta-analysis was updated post the 2017 assessment cycle, the estimated mean from the prior distribution was considered unreasonably high, particularly for rockfish species, and the new prior was not approved by the SSC for use in future West Coast rockfish assessments. In the instance that the SSC does not approve a new estimate (or methodology), the approved approach reverts to the previous approved estimate. In this instance that is the prior estimated for use in the 2017 assessment cycle of a mean = 0.72 with a standard deviation of 0.158.\nBelow is detailed information regarding the meta-analysis approach applied between 2007 - 2017:\n\nThe method is fully described in Thorson, Dorn, and Hamel (2018) Fisheries Research\nThe method can be replicated for any data set 2007/2009/2011/2013/2015/2017 using R package {SebastesSteepness}\nThe rationale for excluding a species when developing a prior for a given focal species (termed “Type-C” meta-analysis) is explained in this paper\n\nIf you are assessing any of the following species you will need to obtain a Type-C prior: aurora, black, bocaccio, canary, chilipepper, darkblotched, gopher, splitnose, widow, yelloweye, yellowtail north\n\nThe estimated prior by year was as follows:\n\n2007: mean = 0.58, sd = 0.181\n2009: mean = 0.69, sd = 0.218\n2011: mean = 0.76, sd = 0.170\n2013: mean = 0.78, sd = 0.152\n2015: mean = 0.77, sd = 0.147\n2017: mean = 0.72, sd = 0.158",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "02-model-choices.html#projections-a.k.a.-forecasts",
    "href": "02-model-choices.html#projections-a.k.a.-forecasts",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.10 Projections (a.k.a. Forecasts)",
    "text": "2.10 Projections (a.k.a. Forecasts)\nThe time-varying Pstar for your forecast.ss file can be calculated using PEPtools::get_buffer(),see PEPtools github repo.\nThe default sigma input to that function is 0.5 for Category 1 stocks and 1.0 for Category 2 but if uncertainty in the model (one of model$Pstar_sigma or model$OFL_sigma as output from r4ss::SS_output()–check with SSC folks on which) is larger, use that instead. The pstar is set by the Council and the adopted value can be found online table GMT015. It is recommended to get confirmation from the GMT representative as well.\nMake sure you are using the right control rule for the PFMC in your forecast file: 3 # Control rule method (0: none; 1: ramp does catch=f(SSB), buffer on F; 2: ramp does F=f(SSB), buffer on F; 3: ramp does catch=f(SSB), buffer on catch; 4: ramp does F=f(SSB), buffer on catch)\nFixed catches for first 2 years of forecast (the current year and next) should be provided by GMT rep to STAR Panel. The values are input at the bottome of the SS3 forecast file and should be split by fleet in the model. Adopted OFL/ACL values are in GMT016 table.\nCreating the forecast table of OFLs and ABCs (ACLs if the relative stock size is below the management target) for your assessment document you can use r4ss::SSexecutivesummary() to help with this task. You may want to specify the OFLs (and ACLs) for the next two years: r4ss::SSexecutivesummary(model, forecast_ofl = c(3763, 3563))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modeling choices, notes and best practices</span>"
    ]
  },
  {
    "objectID": "03-stock-synthesis.html",
    "href": "03-stock-synthesis.html",
    "title": "3  Running Stock Synthesis",
    "section": "",
    "text": "3.1 Model setup\nNote that there’s lots more information on the SS3 website: https://nmfs-ost.github.io/ss3-website including the User Manual.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running Stock Synthesis</span>"
    ]
  },
  {
    "objectID": "03-stock-synthesis.html#model-setup",
    "href": "03-stock-synthesis.html#model-setup",
    "title": "3  Running Stock Synthesis",
    "section": "",
    "text": "Run the model without estimating anything at first. The best way to do this is to run as ss -stopph 0 -nohess where stopph (short for “stop phase” is equivalent to setting the maximum phase in the starter file).\nMake sure you pay attention to any notes or warnings in “warning.sso”. If you don’t understand the warnings, find out, but don’t just ignore it.\nDebugging models that don’t run. The first place to look is “echoinput.sso”. Start at the bottom and scan upwards until things start to look right or start at the top and scan downwards until things start to look wrong. It’s often obvious when you have an extra input and things model starts to go awry. Use this information to fix your input files and try again. There are some additional debugging tips in the Stock Synthesis User Manual. Consider also reading the input files into R using r4ss::SS_read() which may help you find mismatched columns or bad values in the inputs.\nOnce the model runs, look at the “.ss_new” files. These files contain rich comments and often better formatting than your own input files. They are also good for debugging, because sometimes a model will run, but the parameter lines are associated with different fleets, or have different roles than you expected. Check the parameter names on the right hand side of “control.ss_new” to make sure everything looks right. You can then either replace your input files with the “.ss_new” files or just copy and paste elements that you want to keep. Note that if you’ve estimated any parameters, then the initial values in “control.ss_new” have been updated to these estimates.\nPull in parameter bounds that are way too wide – if you aren’t anywhere near them during minimization, extremely wide bounds (like -15 to 15 on recruit deviations, or 3 to 31 for log-R0) just slow minimization and may result in poorer convergence properties.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running Stock Synthesis</span>"
    ]
  },
  {
    "objectID": "03-stock-synthesis.html#model-tuning",
    "href": "03-stock-synthesis.html#model-tuning",
    "title": "3  Running Stock Synthesis",
    "section": "3.2 Model tuning",
    "text": "3.2 Model tuning\n\nIndices are typically tuned via the extra standard deviation parameter. There are many reasons to expect that the input uncertainty values on indices of abundance are underestimates of the true uncertainty. Estimating an extra uncertainty parameter has worked well in a number of west coast groundfish assessments. However, the 2021 best practices document [BUT REVISIONS TO THIS IN PREP FOR 2025] says, “STATs should be cautious to avoid adding variability to an index as a means of resolving model structure issues such as conflicts among data sources. Rather, variability should be added to account for sampling variance underestimating index uncertainty. STATs should provide a priori reasons for why the index variability input to the model has been underestimated (or underspecified).” Note that the extra SD parameter should reflect the observed variability in survey indices rather than poor fit to the observed trend in survey indices. Resist adding SD to surveys where there are trends in residuals without evidence of hyperdepletion or hyperstability, in which case a non-linear relationship between indices and stock size is more appropriate.\nComposition data is typically tuned by either iterative Francis weighting or estimating Dirichlet-multinomial parameters. The McAllister-Ianelli method has not performed as well in simulation testing. See the data weighting section of the Stock Synthesis Manual for more info (Dirichlet-multinomial guidance was updated by Jim Thorson in September 2022). The choice between these two methods often depends on qualitative evaluation of fits to the comp data and index data and how it differs among tuning approaches.\nDiscard ratios and mean body weight These data SHOULD be tuned if they are used in the model, but haven’t often done so. In August 2023 the r4ss::SS_output() function was augmented to return $discard_tuning_info and $mnwgt_tuning_info based on code written by Kelli Johnson for the 2019 sablefish assessment. The following description for tuning was provided in the 2019 assessment of sablefish:\n\nAdded variances for discard rates and mean body weights were set using values calculated iteratively using the RMSE of differences between input and estimated values derived from SS3. Variances were parameterized in terms of standard deviation and coefficient of variation, respectively. The values in the “added” column of the r4ss output can be used in the control file as “Input variance adjustments factors” associated with factors 2 = add_to_discard_stddev or 3 = add_to_bodywt_CV. Eventually these tunings can be added to the r4ss::tune_comps() but as of March 2025 this hasn’t been done.\n\nThink about sigmaR. This could be an arbitrarily chosen value, freely estimated, or iteratively tuned. Methot and Taylor (2011) suggest a way that the tuning could be done. Whatever you choose, put a little thought into it. SigmaR should be greater than the SD of the estimated recruitment deviations. The table $sigma_R_info output by r4ss::SS_output() provides information on tuning sigmaR.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Running Stock Synthesis</span>"
    ]
  },
  {
    "objectID": "04-post-model.html",
    "href": "04-post-model.html",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "",
    "text": "4.1 Retrospectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>So you ran an assessment model, more notes and best practices</span>"
    ]
  },
  {
    "objectID": "04-post-model.html#retrospectives",
    "href": "04-post-model.html#retrospectives",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "",
    "text": "Retrospective analysis is a way to assess whether something is inconsistent in data or model assumptions. A retrospective pattern can arise from changes in catch, M, q, selectivity, or growth from that which is in the model. It is primarily an exploratory/diagnostic tool, though see point 4 below. The {nwfscDiag} package can help with this.\nSee the 11/17/2021 PEP team meeting presentation for background information and past publications on retrospective analysis\nPEP’s approach is to apply:\n\n5 years of peels (based on findings from Miller and Legault 2017),\nusing the alternative Mohn’s rho (Mohn’s rho averaged over peels - the AFSC_Hurtado term from r4ss::SSmohnsrho),\nfor depletion, biomass, fishing mortality, and recruitment. Often biomass, depletion, and recruitment are provided in assessment reports\n\n\n\n\nAlthough the east coast uses mohn’s rho to “correct” status indicators or when setting quotas, our practice is not to. This is based on past precedent, and that our fishing history is not as long as it is on the east coast.\nHurtado-Ferro et al. (2015) provide a rule of thumb on the significance of mohn’s rho (average over peels) dependent on life history. They suggest a retrospective pattern is not meaningful if \\(\\rho \\in (-0.15, 0.2)\\) for long lived species and \\(\\rho \\in (-0.22, 0.3)\\) for short lived species, and note that magnitude of Mohn’s rho not related to true bias in assessment. Miller and Legault (2017) argue that variance of Mohn’s rho is truly needed to ascertain whether an effect is significant.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>So you ran an assessment model, more notes and best practices</span>"
    ]
  },
  {
    "objectID": "04-post-model.html#conducting-a-rebuilding-analysis-aka-puntalyzer",
    "href": "04-post-model.html#conducting-a-rebuilding-analysis-aka-puntalyzer",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "4.2 Conducting a rebuilding analysis (aka Puntalyzer)",
    "text": "4.2 Conducting a rebuilding analysis (aka Puntalyzer)\nA rebuilding plan will need to be developed for any species (or stock) that has an GFSC SSC approved stock assessment that estimates the relative spawning biomass to be below the corresponding minimum stock size threshold (0.10 for flatfish and 0.25 for all other groundfish species). A rebuilding analysis is developed using a software program called the “rebuilder” (aka the Puntalyzer) developed and maintained by Andre Punt (aepunt@uw.edu). The program is designed to work with an input rebuilding data file created by Stock Synthesis called “rebuild.dat”. The rebuilder software executable then develops numerous future projections and calculates the probability of rebuilding based on alternative harvest strategies. The RES.CSV created by the rebuilder executable contains all of the resulting estimates of rebuilding (although many quantities are not labeled within the CSV file).\nThe rebuilder github repository (pfmc-assessment/rebuilder) contains the most up-to-date rebuilder executable, the user manual, code tools for processing output, and code examples. This repository should have all the pertinent information to conduct a rebuilding analysis for a species or stock managed by the Pacific Fishery Management Council",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>So you ran an assessment model, more notes and best practices</span>"
    ]
  },
  {
    "objectID": "04-post-model.html#r4ss",
    "href": "04-post-model.html#r4ss",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "4.3 r4ss",
    "text": "4.3 r4ss\n\nThe {r4ss} package should be installed from GitHub. Installation instructions are available at https://r4ss.github.io/r4ss/.\nSee the intro vignette at on that website for info on more advanced topics like scripting Stock Synthesis workflows with r4ss and using the r4ss::tune_comps() function.\nIt’s good idea to reinstall r4ss on a regular basis to keep up. You can also watch the github repository if you want to get notifications of changes.\nLook through all the figures that get produced by r4ss::SS_plots(). There have been cases where models were put forth with incorrect assumptions about biology, ageing error, etc. that could have been caught if the assessment authors had paid attention to all the plots.\nRemember that the figures can be modified to look better. For instance, you can replace the fleet names in the model with a less abbreviated set of names that will go in the plot labels.\nIf something is not working right, complain about it on the r4ss issues list.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>So you ran an assessment model, more notes and best practices</span>"
    ]
  },
  {
    "objectID": "04-post-model.html#writing-assessment-documents",
    "href": "04-post-model.html#writing-assessment-documents",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "4.4 Writing assessment documents",
    "text": "4.4 Writing assessment documents\n\nRead the terms of reference and actually follow them.\nAs of winter 2024, we are transitioning from {sa4ss} to {asar} but don’t yet have any regional guidance to add to this handbook.\nUse proper dashes and hyphens for readability: Hyphen, En dash, and Em dash\nJason Cope suggests comparing assessment results with estimates from simpler methods saying: “It would be really easy to compare results of every assessment we do with the results of the data-poor methods.” Jason did this for cabezon in 2019 and thinks we could do it for all our assessments.\nThe Council website has a useful dashboard for information on recent fishery performance, OFLs, ACLs, etc. Go to the “GMT016 - Stock Summary” table and filter by species and area.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>So you ran an assessment model, more notes and best practices</span>"
    ]
  },
  {
    "objectID": "05-other.html",
    "href": "05-other.html",
    "title": "5  Other useful tidbits",
    "section": "",
    "text": "5.1 Notes on shared software",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Other useful tidbits</span>"
    ]
  },
  {
    "objectID": "05-other.html#notes-on-shared-software",
    "href": "05-other.html#notes-on-shared-software",
    "title": "5  Other useful tidbits",
    "section": "",
    "text": "5.1.1 Using GitHub\nGit is a version control system that can be used to synchronize files between the web and local directories. GitHub is a user-friendly website which hosts files and depends on Git.\n\n\n5.1.2 Shared code locations\nMost of the R packages and code used in NWFSC assessments are listed in the README at https://github.com/pfmc-assessments. Other sources not included in this github organization include Stock Synthesis and associated packages including {r4ss} and {ss3diags} (not yet used for PFMC assessments).\nIn addition, NMFS OpenSci has a number of NMFS templates for websites, quarto, Rmd docs, and presentations",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Other useful tidbits</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html",
    "href": "cloud-computing.html",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "",
    "text": "6.1 Pick a VM\nThis is the path that I (Kiva) figured out and it worked for me. I am not an expert on computers, cloud computing, or computer security, so if you find yourself thinking “huh, why is she recommending that and not this,” there is probably not a very good reason! In other words, please let me know if you think this could be improved.\nYou’re done! It will take a few moments to get going. If after trying it out you don’t like it, no worries! Most of these settings can be changed on the fly!\nI started out with an 8 core VM. This was twice as many as my laptop, so the gains were not huge. For production purposes, I would probably opt for 16 cores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#pick-a-vm",
    "href": "cloud-computing.html#pick-a-vm",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "",
    "text": "You’ll need an NMFS Azure account. Contact Christine Stawitz to get one.\nLog in.\nClick on Virtual machines -&gt; Create. I think the preset configurations are helpful.\nI’ve been using the D-series\nGive it a name. Other settings I changed from defaults:\n\nSize. Decide how many CPUs (cores) you want. This sets how many stock assessments can be run concurrently.\nUnder networking, add HTTP (80) and HTTPS (443) to the inbound ports. Ignore the warning.\n\nOnce you get to the end of all of the options, mostly sticking to defaults, click “Create.”\nYou’ll need to create a new SSH key for your VM. Once again, just click on the default buttons to keep things advancing, letting the many settings you don’t entirely understand wash over you. (You probably created an SSH key at some point for github.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#connect-to-your-vm",
    "href": "cloud-computing.html#connect-to-your-vm",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.2 Connect to your VM",
    "text": "6.2 Connect to your VM\nThere are several ways to do this. This is what I do:\nGo to the Azure home portal. Under “Resources” your VM should be there. Click on it. If the “Start” button is blue, click on it. This is sort of like turning on your computer. Creating a new VM will automatically turn it on.\nClick on “Connect.” I use the Native SSH. Copy your SSH key’s file name to step 3 of the Native SSH directions. It is probably something like [VM name]_key.pem. It will give you the SSH command to run; copy it.\nOpen up Powershell, Git Bash, or similar. Change the directory (using cd) to wherever you stored your SSH key. If your computer asks if you are sure you want to do this, you bet you are sure.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#install-software",
    "href": "cloud-computing.html#install-software",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.3 Install software",
    "text": "6.3 Install software\nFirst, install some software you will need for cloud computing by running the following lines in the command line:\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get install libxml2 libxml2-dev # igraph\nsudo apt-get install libcairo2-dev # Graphics packages\nsudo apt-get install libssl-dev libcurl4-openssl-dev #httr\nsudo apt-get install apache2\nTo make sure everything is working, in your browser window, navigate to your VM’s IP address, which you can find in the Azure portal of your browser. You should see:\n\nNow install R:\n# update indices\nsudo apt update -qq\n# install two helper packages we need\nsudo apt install --no-install-recommends software-properties-common dirmngr\n# add the signing key (by Michael Rutter) for these repos\n# To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc \n# Fingerprint: E298A3A825C0D65DFD57CBB651716619E084DAB9\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed\nsudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\n# install R\nsudo apt install --no-install-recommends r-base\nThis installed the latest R version as of 10/30/23 (I ended up with 4.3.1). For up-to-date instructions, go to the R project website.\nInstall Rstudio Server:\nTo download the latest version of Rstudio Server, follow the directions here. If you followed the earlier specifications to set up the VM, you should select the operating system “Debian / Ubuntu.” The Ubuntu version that is installed shows up when you first SSH into the VM. For me, it was 20. You’ve already installed R, so scroll down to step 3. It will direct you to run\nsudo apt-get install gdebi-core\nAnd then a couple more lines specific to the version.\nChristine Stawitz says “If you don’t like RStudio as an IDE, there are other options like Code Server (cloud version of VSCode) or Jupyter notebooks.”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#connect-to-rstudio",
    "href": "cloud-computing.html#connect-to-rstudio",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.4 Connect to Rstudio",
    "text": "6.4 Connect to Rstudio\nIn the Azure portal where you can see the VM settings, in the left bar under “Networking” select “Network settings.” Add an inbound port rule. The destination port rule should be 8787 (the default for Rstudio). Keep everything else as a default. As a test, on your browser, navigate to [VM IP address]:8787. You should see an Rstudio login.\nI’m not sure what the correct login is, so let’s change it. From your VM’s Azure portal in your browser, under “Help,” click on “Reset password.” Click the “reset password” bubble. Pick a username and password.\nNow return to that Rstudio login on your browser. Refresh, and enter your username and password you just chose. It should look just like a normal Rstudio window!\nIan: I was actually following the directions here, and then they got really long and I had something working so I stopped. Christine, independently, said she stopped at the same spot. IT may not like accessing it from the public IP address, but she said she is going to do it this way until she gets in trouble.\n\n6.4.1 A note on Rstudio\nRstudio cannot parallel process using a “multicore” strategy, only “multisession.” The makes no difference on your Windows laptop, because Windows also does not support multicore parallel processing. However, if you have set up a Linux VM, it is theoretically an option. I do not know if it is faster or how much.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#install-packages",
    "href": "cloud-computing.html#install-packages",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.5 Install packages",
    "text": "6.5 Install packages\nThings should be looking more familiar! From the Rstudio (or your favorite IDE) tab in your browser, run:\ninstall.packages('remotes')\n\nremotes::install_github('r4ss/r4ss')\n\n# Plus any others you want, e.g., nwfscDiag, here, tidyverse, etc.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#move-files",
    "href": "cloud-computing.html#move-files",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.6 Move files",
    "text": "6.6 Move files\nThere are several ways to do this. I am giving you directions for one approach, using Azcopy. To install Azcopy on your VM, run:\nwget https://aka.ms/downloadazcopy-v10-linux\ntar -xvf downloadazcopy-v10-linux\nsudo cp ./azcopy_linux_amd64_*/azcopy /usr/bin/\nNow\n\nIn the Azure portal of your browser, create a storage account. Title it whatever you want, maybe your name or the name of the assessment. Default settings should be fine.\nOn the left column under “Data storage,” click on “Containers.” Make a new container. Pick a name. Again, default settings are fine.\nNavigate to your new container and on the top bar click on “Upload.” Upload whatever files you want to transfer.\nIn the same container, click on “Shared access signature” under “Security + networking.” I gave it all 7 permissions. Give yourself a little flexibility on the start and expiry date/time. I usually just subtract one day from the start and add one day to the end. You can also enter your VM’s IP address if you want.\nClick “Generate SAS token and URL” and copy the text for the Blob SAS URL.\nBack in your VM terminal, run\n\nsudo azcopy copy \"[paste Blob SAS URL]\" \".\" --recursive\nThe general syntax is:\nsudo azcopy copy \"[copy from]\" \"[copy to]\" --recursive\nThe recursive addition is necessary if multiple files are involved. The files should be downloaded into a new folder with the same name as your container!\n\nBy default, the directory you just created does not have sufficient file permissions to run stock synthesis. To change this type:\n\nsudo chmod 777 [model_directory]\nIf you were transferring a stock assessment model, try running stock synthesis (probably using -nohess) to make sure it is working. To ensure you have the right executable, you can run r4ss::get_ss_exe() from Rstudio (or whatever cloud IDE you are using), and it will automatically download the correct executable to your selected working directory.\n\n6.6.1 Other file transfer methods that you might consider\nYou could install git and clone your repo. I did not look into that because repos can get large and take a while to clone. You can easily download model files straight from github without git using wget, but will eventually need to write the results somewhere.\nAnother approach is to use the scp command from your local computer and avoid the blob storage and azcopy. I think you would have to open up the appropriate port on your VM. Hem, who uses Azure to run Atlantis, recommended the azcopy approach, so that is what I tried. I have used scp in the past on university computing clusters, and it may be fewer steps for each file transfer. I have no idea whether we can run scp on our federal laptops.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#make-sure-you-log-off-your-vm",
    "href": "cloud-computing.html#make-sure-you-log-off-your-vm",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.7 Make sure you log off your VM!!!",
    "text": "6.7 Make sure you log off your VM!!!\nHit “Stop” on the main Azure panel for your VM in your browser. This is akin to shutting down your computer. In general, cloud computing is paid by the minute. The per minute cost is low, but you don’t want to keep it on, e.g., overnight, if you are not using it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "cloud-computing.html#some-thoughts",
    "href": "cloud-computing.html#some-thoughts",
    "title": "6  Cloud computing: jittering a stock assessment",
    "section": "6.8 Some thoughts",
    "text": "6.8 Some thoughts\nThe strongest use case for this in production stock assessments is running jitters. The first time, this requires more active time on your part than running a jitter on your local computer. However, you should get the jitter results faster once it starts running. I ran 8 jitters all at once and it took X min of processing time! Once you set up your VM, you can get back on it whenever you want and your files and settings will be retained, so it is just a matter of transferring files and hitting run.\nNote that {r4ss} is newly set up so that you can run jitters and retrospectives in parallel on your laptop, too! Your laptop just has fewer cores, so you can run fewer models at once. Profiles are not currently set up to run in parallel because it is often useful for convergence purposes to run them sequentially and then use the previous par file as the starter.\nThe VM will not run a single stock synthesis model any faster than your laptop. The gains are really in the number of models that you can run simultaneously.\nAnother use case for this is if you want to rerun all of your sensitivities on a new (e.g., post-STAR) base model. You could do something like furrr::future_walk(dir_string, r4ss::run). For research (e.g., simulations, variable selection, cross validation), the use cases are endless!\nI can fumble my way through file management from the command line, but I don’t love it. Normally we all probably think of Rstudio as an environment for writing and executing R code, but it does provide a nice graphical file manager that I have been making use of. You can also open and edit any text file, including all of the inputs and outputs for stock synthesis. (Other IDEs should provide this functionality, too.) Your Ubuntu VM does come with Vim.\nUntil June, 2024, my understanding is this is all free. After that, we may need to pay. Christine Stawitz thinks it will be possible to pay for the credits in advance when funds are available, and use the credits whenever we want. (Cloud computing in general definitely works that way. It is possible the federal bureacracy ruins it, like all nice things.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cloud computing: jittering a stock assessment</span>"
    ]
  },
  {
    "objectID": "appendix-a.html",
    "href": "appendix-a.html",
    "title": "Appendix A — Species seen* in the hake fishery",
    "section": "",
    "text": "See note about ASHOP program under data sources. The at-sea hake fishery does not fall under the WCGOP program.\n*not a definitive list\n\n\n\n\n\n\n\n\nROCKFISH\n\nFLATFISH\n\n\n\n\n\n\n\n\n\n\nCOMMON\n\n\nCOMMON\n\n\naurora\n\n\narrowtooth flounder\n\n\nbank\n\n\ndover sole\n\n\nblackspotted\n\n\nEnglish sole\n\n\nbocaccio\n\n\nflathead sole\n\n\ncanary\n\n\nPacific halibut\n\n\nchilipepper\n\n\nPacific sanddab\n\n\ndarkblotched\n\n\npetrale sole\n\n\ngreenstriped\n\n\nrex sole\n\n\nPOP\n\n\nslender sole\n\n\nredstripe\n\n\nsouthern rock sole\n\n\nrougheye\n\n\n\n\n\n\nsharpchin\n\n\nUNCOMMON\n\n\nshortbelly\n\n\nCalifornia halibut\n\n\nshortspine\n\n\n\n\n\n\nsilvergray\n\n\n\n\n\n\nsplitnose\n\n\nSHARKS, SKATES, RAYS & RATFISH\n\n\nstripetail\n\n\n\n\n\n\nwidow\n\n\nCOMMON\n\n\nyellowtail\n\n\nbig skate\n\n\n\n\n\n\nblue shark\n\n\nUNCOMMON\n\n\nbrown cat shark\n\n\nblack\n\n\nlongnose skate\n\n\nblackgill\n\n\nPacific electric ray\n\n\nblue\n\n\nsalmon shark\n\n\nharlequin\n\n\nsandpaper skate\n\n\nlongspine\n\n\nsix gill shark\n\n\nquillback\n\n\nsoupfin shark\n\n\nredbanded\n\n\nspiny dogfish shark\n\n\nrosethorn\n\n\nspotted ratfish\n\n\nspeckled\n\n\nthresher shark\n\n\nsquarespot\n\n\n\n\n\n\ntiger\n\n\n\n\n\n\nyelloweye\n\n\nMISCELLANEOUS\n\n\nyellowmouth\n\n\n\n\n\n\n\n\n\n\nCOMMON\n\n\n\n\n\n\nAmerican shad\n\n\nSALMON\n\n\nbarracudinas\n\n\n\n\n\n\njack mackeral\n\n\nCOMMON\n\n\nking-of-the-salmon\n\n\nChinook\n\n\nlamprey\n\n\nchum\n\n\nlanternfish\n\n\ncoho\n\n\nlingcod\n\n\npink (odd years)\n\n\nPacific cod\n\n\n\n\n\n\nPacific herring\n\n\nUNCOMMON\n\n\nPacific mackeral\n\n\nsockeye\n\n\nPacific sardine\n\n\nsteelhead\n\n\npollock\n\n\n\n\n\n\nragfish\n\n\n\n\n\n\nsablefish",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Species seen\\* in the hake fishery</span>"
    ]
  },
  {
    "objectID": "appendix-b.html",
    "href": "appendix-b.html",
    "title": "Appendix B — Alaska Center CRUISEJOINS for shelf and slope surveys",
    "section": "",
    "text": "This list is from AFSC, updated with 2004 info by John Wallace\n\n\n\n\n\n\n\nTRIENNIAL SURVEY CRUISEJOINS\n\n\n\nYEAR\nCRUISEJOINS\n\n\n1977\n393 (Commando), 421 (Pacific Raider), 423 (Jordan), 500 (Tordenskjold)\n\n\n1980\n394 (Mary Lou), 404 (Pat San Marie)\n\n\n1983\n433 (Warrior II), 434 (Nordfjord)\n\n\n1986\n406 (Pat San Marie), 429 (Alaska)\n\n\n1989\n407 (Pat San Marie), 461 (Golden Fleece)\n\n\n1992\n432 (Alaska), 465 (Green Hope)\n\n\n1995\n852417 (Alaska), 852418 (Vesteraalen)\n\n\n1998\n921326 (Dominator), 929471 (Vesteraalen)\n\n\n2001\n1090096 (Sea Storm), 1090095 (Frosti)\n\n\n2004\n1236675, 1236676 Morning Star Vesteraalen\n\n\nSLOPE SURVEY CRUISEJOINS\n\n\n\n\nYEAR\nCRUISEJOINS\n\n\n1984\n435 (Half Moon Bay)\n\n\n1988\n413 (Freeman)\n\n\n1989\n462 (Golden Fleece)\n\n\n1990\n416 (Freeman)\n\n\n1991\n417 (Freeman)\n\n\n1992\n418 (Freeman)\n\n\n1993\n419 (Freeman)\n\n\n1995\n852455 (Freeman)\n\n\n1996\n869365 (Freeman)\n\n\n1997\n912730 (Freeman)\n\n\n1999\n998132 (Freeman)\n\n\n2000\n1028815 (Freeman)\n\n\n2001\n1105595 (Freeman)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Alaska Center CRUISEJOINS for shelf and slope surveys</span>"
    ]
  }
]